{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463ad8b1-ba98-4c28-ab95-58c5ae06d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7a4dd7e-47de-4b19-9218-650139d8d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4fa0c5-cdbb-4542-94cc-9b8cfd5da25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from self_supervision.estimator.cellnet import EstimatorAutoEncoder\n",
    "from self_supervision.paths import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7f53c2-66ec-4bbe-b5ca-42e8f0edbbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_DIR = os.path.join(DATA_DIR, 'merlin_cxg_2023_05_15_sf-log1p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f8eb6a-3d53-40aa-b437-b19f2f553c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init estim class\n",
    "estim = EstimatorAutoEncoder(STORE_DIR, hvg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9977650a-68a3-4e9f-b9d5-62feb24c6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init datamodule\n",
    "estim.init_datamodule(batch_size=4096) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae2b215-f93d-4539-8dba-67c805a08865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init random clf model\n",
    "estim.init_model(\n",
    "                model_type='mlp_clf',\n",
    "                model_kwargs={\n",
    "                    'learning_rate': 1e-3,\n",
    "                    'weight_decay': 0.1,\n",
    "                    'lr_scheduler': torch.optim.lr_scheduler.StepLR,\n",
    "                    'dropout': 0.1,\n",
    "                    'lr_scheduler_kwargs': {\n",
    "                        'step_size': 2,\n",
    "                        'gamma': 0.9,\n",
    "                        'verbose': True\n",
    "                    },\n",
    "                    'units': [512, 512, 256, 256, 64],\n",
    "                    'supervised_subset': None,\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bfe395f-e5d3-4056-9b82-3bb57a27e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  ({'X': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), 'cell_type': tensor([  3, 136, 122,  ...,  14,  19, 129], device='cuda:0'), 'dataset_id': tensor([168, 169, 168,  ..., 143, 178, 191], device='cuda:0')}, None)\n"
     ]
    }
   ],
   "source": [
    "for batch in estim.datamodule.train_dataloader():\n",
    "    print('batch: ', batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e83c975-4fba-4597-bcf7-8feaabdffc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_samples(dataloader):\n",
    "    \"\"\"\n",
    "    Count the number of samples in a given dataloader.\n",
    "    \"\"\"\n",
    "    sample_count = 0\n",
    "    for batch in dataloader:\n",
    "        sample_count += len(batch[0]['X'])\n",
    "        gc.collect()  # Invoke garbage collector\n",
    "    return sample_count\n",
    "\n",
    "def count_cell_types(dataloader):\n",
    "    \"\"\"\n",
    "    Count the number of unique cell types in a given dataloader.\n",
    "    \"\"\"\n",
    "    unique_cell_types = set()\n",
    "    for batch in dataloader:\n",
    "        unique_cell_types.update(batch[0]['cell_type'].tolist())\n",
    "        gc.collect()  # Invoke garbage collector\n",
    "    return len(unique_cell_types)\n",
    "\n",
    "def filter_by_dataset_id(dataloader, dataset_id):\n",
    "    \"\"\"\n",
    "    Filter the batches by a given dataset_id.\n",
    "    \"\"\"\n",
    "    filtered_batches = []\n",
    "    for batch in dataloader:\n",
    "        mask = batch[0]['dataset_id'] == dataset_id\n",
    "        if any(mask):\n",
    "            filtered_batches.append(({'X': batch[0]['X'][mask], \n",
    "                                      'cell_type': batch[0]['cell_type'][mask], \n",
    "                                      'dataset_id': batch[0]['dataset_id'][mask]}, \n",
    "                                      batch[1]))\n",
    "    return filtered_batches\n",
    "\n",
    "def print_dataset_info(train_dataloader, val_dataloader, test_dataloader, dataset_id=None, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Calculate and print the dataset information.\n",
    "    \"\"\"\n",
    "    if dataset_id is not None:\n",
    "        print(f\"Information for dataset '{dataset_name}' (ID: {dataset_id}):\")\n",
    "        train_dataloader = filter_by_dataset_id(train_dataloader, dataset_id)\n",
    "        val_dataloader = filter_by_dataset_id(val_dataloader, dataset_id)\n",
    "        test_dataloader = filter_by_dataset_id(test_dataloader, dataset_id)\n",
    "    else:\n",
    "        print(\"Information for the complete dataset:\")\n",
    "\n",
    "    train_samples = count_samples(train_dataloader)\n",
    "    val_samples = count_samples(val_dataloader)\n",
    "    test_samples = count_samples(test_dataloader)\n",
    "\n",
    "    train_cell_types = count_cell_types(train_dataloader)\n",
    "    val_cell_types = count_cell_types(val_dataloader)\n",
    "    test_cell_types = count_cell_types(test_dataloader)\n",
    "\n",
    "    print(f\"Train samples: {train_samples}\")\n",
    "    print(f\"Validation samples: {val_samples}\")\n",
    "    print(f\"Test samples: {test_samples}\")\n",
    "\n",
    "    print(f\"Unique cell types in Train set: {train_cell_types}\")\n",
    "    print(f\"Unique cell types in Validation set: {val_cell_types}\")\n",
    "    print(f\"Unique cell types in Test set: {test_cell_types}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ebb34-d833-4531-9438-762be4ee5711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information for the complete dataset:\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "print_dataset_info(estim.datamodule.train_dataloader(), \n",
    "                   estim.datamodule.val_dataloader(), \n",
    "                   estim.datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8be6c7-abbc-4ca6-acd7-ea18313cda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For specific dataset_ids\n",
    "dataset_ids = {'HLCA': 148, 'Tabula Sapiens': 87, 'PBMC': 41}\n",
    "for name, dataset_id in dataset_ids.items():\n",
    "    print_dataset_info(estim.datamodule.train_dataloader(), \n",
    "                       estim.datamodule.val_dataloader(), \n",
    "                       estim.datamodule.test_dataloader(), \n",
    "                       dataset_id, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:celldreamer]",
   "language": "python",
   "name": "celldreamer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
