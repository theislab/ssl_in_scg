{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda2a3ce-03cb-4455-b8b9-746041287b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d17c4f21-a5f3-4370-a0cb-4a97166cdea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, TQDMProgressBar, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3579322a-686d-4c40-8dbd-b09edebe12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from self_supervision.data.checkpoint_utils import load_last_checkpoint, checkpoint_exists\n",
    "from self_supervision.trainer.masking.mask_utils import encode_gene_programs, encode_gene_program_to_transcription_factor, read_gmt_to_dict, read_gmt\n",
    "from self_supervision.estimator.cellnet import EstimatorAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fcbfc6d-4c64-4836-8f28-1f1cd6d80f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your large set of parameters\n",
    "LARGE_PARAMS = {\n",
    "    \"decoder\": False,\n",
    "    \"model\": \"VAE\",\n",
    "    \"mask_rate\": 0.5,\n",
    "    \"masking_strategy\": \"random\",\n",
    "    \"gp_file\": \"C5\",\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"batch_size\": 8,\n",
    "    \"mask_type\": \"sparsemax\",\n",
    "    \"version\": \"\",\n",
    "    \"lr\": 0.1,\n",
    "    \"hidden_units\": [512, 512, 256, 256, 64],\n",
    "    \"checkpoint_interval\": 1,\n",
    "    \"hvg\": False,\n",
    "    \"num_hvgs\": 2000,\n",
    "    \"missing_tolerance\": 0,\n",
    "    \"data_path\": \"/lustre/groups/ml01/workspace/till.richter/merlin_cxg_2023_05_15_sf-log1p\",\n",
    "    \"model_path\": \"/lustre/groups/ml01/workspace/till.richter/\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864ea927-1ece-47b7-b4a4-a8db5d635ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.dirname(os.path.abspath(os.curdir))\n",
    "CHECKPOINT_PATH = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b17b89c-7cc8-4ed4-bf48-7fe8922d5fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# get estimator\n",
    "estim = EstimatorAutoEncoder(data_path=LARGE_PARAMS[\"data_path\"], hvg=LARGE_PARAMS[\"hvg\"])\n",
    "\n",
    "# set up datamodule\n",
    "estim.init_datamodule(batch_size=LARGE_PARAMS[\"batch_size\"])\n",
    "\n",
    "estim.init_trainer(\n",
    "    trainer_kwargs={\n",
    "        'max_epochs': 1000,\n",
    "        'gradient_clip_val': 1.,\n",
    "        'gradient_clip_algorithm': 'norm',\n",
    "        'default_root_dir': CHECKPOINT_PATH,\n",
    "        'accelerator': 'gpu',\n",
    "        'devices': 1,\n",
    "        'num_sanity_val_steps': 0,\n",
    "        'check_val_every_n_epoch': 1,\n",
    "        'logger': [TensorBoardLogger(CHECKPOINT_PATH, name='default')],\n",
    "        'log_every_n_steps': 100,\n",
    "        'detect_anomaly': False,\n",
    "        'enable_progress_bar': True,\n",
    "        'enable_model_summary': False,\n",
    "        'enable_checkpointing': True,\n",
    "        'callbacks': [\n",
    "            TQDMProgressBar(refresh_rate=300),\n",
    "            LearningRateMonitor(logging_interval='step'),\n",
    "            ModelCheckpoint(filename='best_checkpoint_train', monitor='train_loss_epoch', mode='min',\n",
    "                            every_n_epochs=LARGE_PARAMS[\"checkpoint_interval\"], save_top_k=1),\n",
    "            ModelCheckpoint(filename='best_checkpoint_val', monitor='val_loss', mode='min',\n",
    "                            every_n_epochs=LARGE_PARAMS[\"checkpoint_interval\"], save_top_k=1),\n",
    "            ModelCheckpoint(filename='last_checkpoint', monitor=None),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# init model\n",
    "estim.init_model(\n",
    "    model_type='mlp_ae' if LARGE_PARAMS[\"model\"] == 'MLP' else 'mlp_vae',\n",
    "    model_kwargs={\n",
    "        'learning_rate': LARGE_PARAMS[\"lr\"],\n",
    "        'weight_decay': LARGE_PARAMS[\"weight_decay\"],\n",
    "        'dropout': LARGE_PARAMS[\"dropout\"],\n",
    "        'lr_scheduler': torch.optim.lr_scheduler.StepLR,\n",
    "        'lr_scheduler_kwargs': {\n",
    "            'step_size': 2,\n",
    "            'gamma': 0.9,\n",
    "            'verbose': True\n",
    "        },\n",
    "        'masking_strategy': LARGE_PARAMS[\"masking_strategy\"],\n",
    "        'masking_rate': LARGE_PARAMS[\"mask_rate\"],\n",
    "        # 'encoded_gene_program': encoded_gene_program,\n",
    "        'units_encoder': LARGE_PARAMS[\"hidden_units\"],\n",
    "        'units_decoder': LARGE_PARAMS[\"hidden_units\"][::-1][1:] if LARGE_PARAMS[\"decoder\"] else [],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc625cd8-226d-4a2f-ad75-ca7e6fba3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = estim.datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a866e994-4b61-4786-b34e-855bacd5f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  {'X': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), 'cell_type': tensor([ 12,  50, 124,  10, 162, 129, 151, 122], device='cuda:0'), 'dataset_id': tensor([ 35, 220, 168,   3, 124, 147, 160,  41], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for i, batch in dataloader:\n",
    "    print('batch: ', i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9724879f-d109-4c1c-8333-2bdd7c37a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_matrix_and_obs(ddf):\n",
    "    x = (\n",
    "        ddf['X']\n",
    "        .map_partitions(\n",
    "            lambda xx: pd.DataFrame(np.vstack(xx.tolist())), \n",
    "            meta={col: 'f4' for col in range(19331)}\n",
    "        )\n",
    "        .to_dask_array(lengths=[1024] * ddf.npartitions)\n",
    "    )\n",
    "    obs = ddf[['cell_type', 'dataset_id']].compute()\n",
    "    \n",
    "    return x, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d011ff3f-b342-45d2-99aa-d422eb2b0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/lustre/groups/ml01/workspace/till.richter/merlin_cxg_2023_05_15_sf-log1p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b6a82d-9af7-47ae-91b7-c064c267ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(os.path.join(PATH, 'train'), split_row_groups=True)\n",
    "x, obs = get_count_matrix_and_obs(ddf)\n",
    "var = pd.read_parquet(os.path.join(PATH, 'var.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da429c32-0211-4eed-89a2-4669ed281faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = \"9f222629-9e39-47d0-b83f-e08d610c7479\"  # HLCA\n",
    "target_id = \"53d208b0-2cfd-4366-9866-c3c6114081bc\"  # Tabula Sapiens\n",
    "target_id = \"2a498ace-872a-4935-984b-1afa70fd9886\"  # PBMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cd535e3-e12b-42e2-a578-1b5e3900ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id_mapping = pd.read_parquet(os.path.join(PATH, 'categorical_lookup/dataset_id.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db5a8136-52e0-4fd0-ab0d-5c9d118fe934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corresponding int is:  41\n"
     ]
    }
   ],
   "source": [
    "result = dataset_id_mapping[dataset_id_mapping['label'] == target_id]\n",
    "\n",
    "if not result.empty:\n",
    "    corresponding_int = result.index[0]\n",
    "    print('corresponding int is: ', corresponding_int)\n",
    "else:\n",
    "    corresponding_int = None  # or some default value\n",
    "    print('doesnt work')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d79dd9-8932-4f3e-a1ad-bb45682e2cc7",
   "metadata": {},
   "source": [
    "Dataset_ID of the HLCA is encoded as 148\n",
    "\n",
    "Dataset_ID of Tabula Sapiens is encoded as 87\n",
    "\n",
    "Dataset_ID of PBMC is encoded as 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2629ec-2fdd-4705-9fa2-fcee15070ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:celldreamer]",
   "language": "python",
   "name": "celldreamer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
