{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a51d42-a1c8-4aa3-9dcd-e6dce0e1ba52",
   "metadata": {},
   "source": [
    "# From https://github.com/theislab/scTab/blob/devel/notebooks/store_creation/05_compute_pca.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "844fddb8-4dc5-4719-8543-ccff10d61f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dask_ml.decomposition import IncrementalPCA\n",
    "import scanpy as sc\n",
    "import torch\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadc788-394b-4628-b99d-aec2fbf044b1",
   "metadata": {},
   "source": [
    "### CellNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ac5d9a-f108-4bcc-92bf-f2cf93008cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/lustre/groups/ml01/workspace/till.richter/merlin_cxg_2023_05_15_sf-log1p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006eb17f-feca-49bb-bf07-9989f780d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_matrix(ddf):\n",
    "    x = (\n",
    "        ddf['X']\n",
    "        .map_partitions(\n",
    "            lambda xx: pd.DataFrame(np.vstack(xx.tolist())), \n",
    "            meta={col: 'f4' for col in range(19331)}\n",
    "        )\n",
    "        .to_dask_array(lengths=[1024] * ddf.npartitions)\n",
    "    )\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270b8ea-139c-4a8c-af5a-082d6316101c",
   "metadata": {},
   "source": [
    "Compute PCA for consistency with 64 dimensions in trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7550bc9-b4b4-4d66-b09d-5485be07cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split  test already exists\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(join(PATH, 'pca'), exist_ok=True)\n",
    "\n",
    "\n",
    "n_comps = 64\n",
    "\n",
    "\n",
    "for split in ['test']:  # we only need test\n",
    "    if os.path.exists(join(PATH, 'pca', f'x_pca_{split}_{n_comps}.npy')):\n",
    "        print('Split ', split, 'already exists')\n",
    "        continue\n",
    "    x = get_count_matrix(dd.read_parquet(join(PATH, split), split_row_groups=True))\n",
    "    pca = IncrementalPCA(n_components=n_comps, iterated_power=3)\n",
    "    x_pca = da.compute(pca.fit_transform(x))[0]\n",
    "    with open(join(PATH, 'pca', f'x_pca_{split}_{n_comps}.npy'), 'wb') as f:\n",
    "        np.save(f, x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90254538-42f9-449f-b43e-4f7168085511",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.dirname(os.path.dirname(os.path.abspath(os.path.curdir)))\n",
    "hvg_indices = pickle.load(open(root + '/self_supervision/data/hvg_2000_indices.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf43632-49b5-45dd-8152-e73a17a14567",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(join(PATH, 'pca'), exist_ok=True)\n",
    "\n",
    "\n",
    "n_comps = 64\n",
    "\n",
    "\n",
    "for split in ['test']:\n",
    "    if os.path.exists(join(PATH, 'pca', f'x_hvg_pca_{split}_{n_comps}.npy')):\n",
    "        print('Split ', split, 'already exists')\n",
    "        continue\n",
    "    x = get_count_matrix(dd.read_parquet(join(PATH, split), split_row_groups=True))\n",
    "    x = x[:, hvg_indices]\n",
    "    pca = IncrementalPCA(n_components=n_comps, iterated_power=3)\n",
    "    x_pca = da.compute(pca.fit_transform(x))[0]\n",
    "    with open(join(PATH, 'pca', f'x_hvg_pca_{split}_{n_comps}.npy'), 'wb') as f:\n",
    "        np.save(f, x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a6162a-ea37-4225-900c-09536c836608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For subsampled train data\n",
    "\n",
    "def get_sampled_files(base_path, split, sub_sample_frac):\n",
    "    files = [file for file in os.listdir(os.path.join(base_path, split)) if file.endswith('.parquet')]\n",
    "    files = [os.path.join(base_path, split, file) for file in sorted(files, key=lambda x: int(x.split('.')[1]))]\n",
    "    num_files_to_sample = ceil(sub_sample_frac * len(files))\n",
    "    return files[:num_files_to_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8df283a-f5ab-4049-ac17-a942cc16a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_matrix(files):\n",
    "    x = (\n",
    "        dd.read_parquet(files, engine='pyarrow')\n",
    "        .map_partitions(\n",
    "            lambda df: pd.DataFrame(np.vstack(df['X'].tolist())), \n",
    "            meta={col: 'f4' for col in range(19331)}\n",
    "        )\n",
    "        .to_dask_array(lengths=True)\n",
    "    )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "344f9cf1-78b8-4650-976a-98bccce4d2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA for subsampled train data computed and saved.\n"
     ]
    }
   ],
   "source": [
    "n_comps = 64\n",
    "sub_sample_frac = 0.2  # 20% subsample\n",
    "train_files = get_sampled_files(PATH, 'train', sub_sample_frac)\n",
    "\n",
    "# Create a directory for PCA results\n",
    "os.makedirs(os.path.join(PATH, 'pca'), exist_ok=True)\n",
    "\n",
    "# Check if PCA has already been computed for the subsampled train data\n",
    "pca_path = os.path.join(PATH, 'pca', f'x_pca_20p_subsample_train_{n_comps}.npy')\n",
    "if os.path.exists(pca_path):\n",
    "    print('PCA for subsampled train data already exists')\n",
    "else:\n",
    "    x = get_count_matrix(train_files)\n",
    "    pca = IncrementalPCA(n_components=n_comps, iterated_power=3)\n",
    "    # Compute PCA in an incremental way to handle large data\n",
    "    x_pca = da.compute(pca.fit_transform(x))[0]\n",
    "    with open(pca_path, 'wb') as f:\n",
    "        np.save(f, x_pca)\n",
    "    print('PCA for subsampled train data computed and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891df42-f400-47d2-8731-293c3e2e7ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c68ccb6f-f26f-4e4f-9577-bdca9c354723",
   "metadata": {},
   "source": [
    "### OOD - Tail of Hippocampus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14a9fae-0283-4922-841b-0e7d4651467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/lustre/groups/ml01/workspace/till.richter/'\n",
    "adata = sc.read_h5ad(os.path.join(DATA_PATH, 'tail_of_hippocampus', 'tail_of_hippocampus.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0eff802-c6af-45c2-bb53-b51916758bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw adata with shape:  (56367, 59357)\n",
      "Loaded CellNet genes.\n",
      "Found 18956 common genes and 375 missing genes.\n",
      "Filtered and reordered genes. New adata shape:  (56367, 18956)\n",
      "Normalized and log-transformed data.\n",
      "Filtered cells by valid cell types. New adata shape:  (56281, 18956)\n",
      "Final shape of adata:  (56281, 19331)\n"
     ]
    }
   ],
   "source": [
    "# Load adata\n",
    "print('Loaded raw adata with shape: ', adata.X.shape)\n",
    "\n",
    "# Load CellNet genes\n",
    "cellnet_genes_path = os.path.join(PATH, 'var.parquet')\n",
    "cellnet_genes = list(pd.read_parquet(cellnet_genes_path)['feature_id'])\n",
    "print(\"Loaded CellNet genes.\")\n",
    "\n",
    "# Find common and missing genes\n",
    "common_genes = list(set(adata.var['Gene'].index) & set(cellnet_genes))\n",
    "missing_genes = list(set(cellnet_genes) - set(adata.var['Gene'].index))\n",
    "print(f\"Found {len(common_genes)} common genes and {len(missing_genes)} missing genes.\")\n",
    "\n",
    "# Create a dictionary to map 'Gene' to 'ensembl_ids'\n",
    "gene_to_ensembl = dict(zip(adata.var['Gene'].index, adata.var_names))\n",
    "\n",
    "# Convert common genes to their corresponding ensembl IDs\n",
    "common_ensembl_ids = [gene_to_ensembl[gene] for gene in common_genes]\n",
    "\n",
    "# Filter and reorder genes\n",
    "adata = adata[:, common_ensembl_ids]\n",
    "print(\"Filtered and reordered genes. New adata shape: \", adata.X.shape)\n",
    "\n",
    "# Normalize and log transform\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "print(\"Normalized and log-transformed data.\")\n",
    "\n",
    "# Load cell_type_mapping\n",
    "cell_type_mapping = pd.read_parquet(os.path.join(PATH, 'categorical_lookup/cell_type.parquet'))\n",
    "\n",
    "# Create mapping dictionary for cell_type to int64 encoding\n",
    "cell_type_to_encoding = {cell_type: idx for idx, cell_type in cell_type_mapping['label'].items()}\n",
    "\n",
    "# Filter cells by valid cell types\n",
    "valid_cell_types = set(cell_type_to_encoding.keys())\n",
    "adata = adata[adata.obs['cell_type'].isin(valid_cell_types)]\n",
    "print(\"Filtered cells by valid cell types. New adata shape: \", adata.X.shape)\n",
    "\n",
    "# Encode cell types\n",
    "y_adata = np.array([cell_type_to_encoding[cell_type] for cell_type in adata.obs['cell_type'].values])\n",
    "\n",
    "# Zero-padding\n",
    "if missing_genes:\n",
    "    zero_padding_df = pd.DataFrame(\n",
    "        data=0,\n",
    "        index=adata.obs.index,\n",
    "        columns=missing_genes\n",
    "    )\n",
    "\n",
    "    concatenated_df = pd.concat([adata.to_df(), zero_padding_df], axis=1)\n",
    "    concatenated_df = concatenated_df[cellnet_genes]  # Ensure ordering of genes\n",
    "\n",
    "    # Create new AnnData object to ensure consistency\n",
    "    adata = sc.AnnData(X=concatenated_df.values, \n",
    "                        obs=adata.obs,\n",
    "                        var=pd.DataFrame(index=cellnet_genes))\n",
    "\n",
    "# Double-check that the genes are in the correct order\n",
    "assert all(adata.var_names == cellnet_genes), 'Genes are not in the correct order.'\n",
    "\n",
    "print('Final shape of adata: ', adata.X.shape)\n",
    "\n",
    "\n",
    "# PyTorch DataLoader\n",
    "# Assuming you have a function called `cell_type_to_encoding` to convert cell_type to int64\n",
    "tensor_x = torch.Tensor(adata.X)\n",
    "tensor_y = torch.Tensor(adata.obs['cell_type'].map(cell_type_to_encoding).values).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbf82c5-54b2-403b-af23-29bb8a659788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56281, 19331])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5decc23-6636-493f-9648-3a5385f9aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "x_pca = pca.fit_transform(tensor_x)\n",
    "with open(join(DATA_PATH, 'tail_of_hippocampus', 'pca', f'x_tail_of_hippocampus_pca_64.npy'), 'wb') as f:\n",
    "        np.save(f, x_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982e6df-64c2-4385-ab15-44b2df7e56f4",
   "metadata": {},
   "source": [
    "### OOD - Non neuronal cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d473cd71-8b53-4c52-b10b-4a2ed4bf2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/lustre/groups/ml01/workspace/till.richter/'\n",
    "adata = sc.read_h5ad(os.path.join(DATA_PATH, 'non_neuronal', 'non_neuronal.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7b2e71-b229-4cc7-baee-61d763acde4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw adata with shape:  (888263, 59357)\n",
      "Loaded CellNet genes.\n",
      "Found 18956 common genes and 375 missing genes.\n",
      "Filtered and reordered genes. New adata shape:  (888263, 18956)\n",
      "Normalized and log-transformed data.\n",
      "Filtered cells by valid cell types. New adata shape:  (871418, 18956)\n",
      "Final shape of adata:  (871418, 19331)\n"
     ]
    }
   ],
   "source": [
    "# Load adata\n",
    "print('Loaded raw adata with shape: ', adata.X.shape)\n",
    "\n",
    "# Load CellNet genes\n",
    "cellnet_genes_path = os.path.join(PATH, 'var.parquet')\n",
    "cellnet_genes = list(pd.read_parquet(cellnet_genes_path)['feature_id'])\n",
    "print(\"Loaded CellNet genes.\")\n",
    "\n",
    "# Find common and missing genes\n",
    "common_genes = list(set(adata.var['Gene'].index) & set(cellnet_genes))\n",
    "missing_genes = list(set(cellnet_genes) - set(adata.var['Gene'].index))\n",
    "print(f\"Found {len(common_genes)} common genes and {len(missing_genes)} missing genes.\")\n",
    "\n",
    "# Create a dictionary to map 'Gene' to 'ensembl_ids'\n",
    "gene_to_ensembl = dict(zip(adata.var['Gene'].index, adata.var_names))\n",
    "\n",
    "# Convert common genes to their corresponding ensembl IDs\n",
    "common_ensembl_ids = [gene_to_ensembl[gene] for gene in common_genes]\n",
    "\n",
    "# Filter and reorder genes\n",
    "adata = adata[:, common_ensembl_ids]\n",
    "print(\"Filtered and reordered genes. New adata shape: \", adata.X.shape)\n",
    "\n",
    "# Normalize and log transform\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "print(\"Normalized and log-transformed data.\")\n",
    "\n",
    "# Load cell_type_mapping\n",
    "cell_type_mapping = pd.read_parquet(os.path.join(PATH, 'categorical_lookup/cell_type.parquet'))\n",
    "\n",
    "# Create mapping dictionary for cell_type to int64 encoding\n",
    "cell_type_to_encoding = {cell_type: idx for idx, cell_type in cell_type_mapping['label'].items()}\n",
    "\n",
    "# Filter cells by valid cell types\n",
    "valid_cell_types = set(cell_type_to_encoding.keys())\n",
    "adata = adata[adata.obs['cell_type'].isin(valid_cell_types)]\n",
    "print(\"Filtered cells by valid cell types. New adata shape: \", adata.X.shape)\n",
    "\n",
    "# Encode cell types\n",
    "y_adata = np.array([cell_type_to_encoding[cell_type] for cell_type in adata.obs['cell_type'].values])\n",
    "\n",
    "# Zero-padding\n",
    "if missing_genes:\n",
    "    zero_padding_df = pd.DataFrame(\n",
    "        data=0,\n",
    "        index=adata.obs.index,\n",
    "        columns=missing_genes\n",
    "    )\n",
    "\n",
    "    concatenated_df = pd.concat([adata.to_df(), zero_padding_df], axis=1)\n",
    "    concatenated_df = concatenated_df[cellnet_genes]  # Ensure ordering of genes\n",
    "\n",
    "    # Create new AnnData object to ensure consistency\n",
    "    adata = sc.AnnData(X=concatenated_df.values, \n",
    "                        obs=adata.obs,\n",
    "                        var=pd.DataFrame(index=cellnet_genes))\n",
    "\n",
    "# Double-check that the genes are in the correct order\n",
    "assert all(adata.var_names == cellnet_genes), 'Genes are not in the correct order.'\n",
    "\n",
    "print('Final shape of adata: ', adata.X.shape)\n",
    "\n",
    "\n",
    "# PyTorch DataLoader\n",
    "# Assuming you have a function called `cell_type_to_encoding` to convert cell_type to int64\n",
    "tensor_x = torch.Tensor(adata.X)\n",
    "tensor_y = torch.Tensor(adata.obs['cell_type'].map(cell_type_to_encoding).values).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642ad760-3d68-4e1a-abbe-08d0a2483bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([871418, 19331])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b31cca5-8712-46df-9169-bbb77d8db3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "x_pca = pca.fit_transform(tensor_x)\n",
    "with open(join(DATA_PATH, 'non_neuronal', 'pca', 'x_non_neuronal_pca_64.npy'), 'wb') as f:\n",
    "        np.save(f, x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40daa6ed-b2f0-49b9-b50f-3a1fb8dff166",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dask = da.from_array(tensor_x, chunks=(1000, x.shape[1]))\n",
    "x_dask = x_dask.astype('float64')\n",
    "\n",
    "# Initialize Incremental PCA\n",
    "ipca = IncrementalPCA(n_components=64)\n",
    "\n",
    "# Fit and Transform\n",
    "x_pca_dask = ipca.fit_transform(x_dask)\n",
    "\n",
    "# Save to disk\n",
    "with open(join(DATA_PATH, 'non_neuronal', 'pca', 'x_non_neuronal_pca_64.npy'), 'wb') as f:\n",
    "    np.save(f, x_pca_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361aa16-21ad-4d4e-876f-adc0a952e91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8613c-9989-482f-938a-1c7b45ba27d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:celldreamer]",
   "language": "python",
   "name": "celldreamer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
